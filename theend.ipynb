{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13949971,"sourceType":"datasetVersion","datasetId":8891486}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"khadijamouhta/embedingsctr\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Cloner ton repo\n!git clone https://github.com/khadijamouhtaj55-blip/Competition.git\n%cd Competition\n\n# 2. Installer les d√©pendances minimales\n!pip install fuxictr==2.3.7\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:31:49.830135Z","iopub.execute_input":"2025-12-13T12:31:49.830854Z","iopub.status.idle":"2025-12-13T12:31:55.429507Z","shell.execute_reply.started":"2025-12-13T12:31:49.830825Z","shell.execute_reply":"2025-12-13T12:31:55.428308Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Competition'...\nremote: Enumerating objects: 43, done.\u001b[K\nremote: Counting objects: 100% (43/43), done.\u001b[K\nremote: Compressing objects: 100% (32/32), done.\u001b[K\nremote: Total 43 (delta 12), reused 35 (delta 8), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (43/43), 64.26 KiB | 2.47 MiB/s, done.\nResolving deltas: 100% (12/12), done.\n/kaggle/working/Competition\nCollecting fuxictr==2.3.7\n  Downloading fuxictr-2.3.7-py3-none-any.whl.metadata (29 kB)\nCollecting keras-preprocessing (from fuxictr==2.3.7)\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fuxictr==2.3.7) (2.2.3)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from fuxictr==2.3.7) (6.0.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from fuxictr==2.3.7) (1.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fuxictr==2.3.7) (1.26.4)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from fuxictr==2.3.7) (3.14.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fuxictr==2.3.7) (4.67.1)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from fuxictr==2.3.7) (19.0.1)\nRequirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (from fuxictr==2.3.7) (1.25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->fuxictr==2.3.7) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->fuxictr==2.3.7) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->fuxictr==2.3.7) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->fuxictr==2.3.7) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->fuxictr==2.3.7) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->fuxictr==2.3.7) (2.4.1)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from keras-preprocessing->fuxictr==2.3.7) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->fuxictr==2.3.7) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->fuxictr==2.3.7) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fuxictr==2.3.7) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fuxictr==2.3.7) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fuxictr==2.3.7) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fuxictr==2.3.7) (3.6.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fuxictr==2.3.7) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fuxictr==2.3.7) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fuxictr==2.3.7) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->fuxictr==2.3.7) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->fuxictr==2.3.7) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->fuxictr==2.3.7) (2024.2.0)\nDownloading fuxictr-2.3.7-py3-none-any.whl (88 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: keras-preprocessing, fuxictr\nSuccessfully installed fuxictr-2.3.7 keras-preprocessing-1.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install gensim\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:10:06.111272Z","iopub.execute_input":"2025-12-13T14:10:06.111741Z","iopub.status.idle":"2025-12-13T14:10:10.014417Z","shell.execute_reply.started":"2025-12-13T14:10:06.111713Z","shell.execute_reply":"2025-12-13T14:10:10.013486Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.4.0)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\nRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.15.3)\nRequirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (2.4.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open>=1.8.1->gensim) (1.17.2)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->gensim) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->gensim) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->gensim) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->gensim) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.5->gensim) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.5->gensim) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel\nfrom sklearn.decomposition import PCA\nfrom gensim.models import Word2Vec \n\n# =============================================================================\n# CONFIGURATION\n# =============================================================================\nDATASET_ROOT = \"/kaggle/input/embedingsctr\"\nIMG_DIR = os.path.join(DATASET_ROOT, \"item_images\")\nITEM_INFO_PATH = os.path.join(DATASET_ROOT, \"item_info (1).parquet\")\nTRAIN_PATH = os.path.join(DATASET_ROOT, \"train.parquet\")\nOUTPUT_PATH = \"/kaggle/working/item_info_updated.parquet\"\n\nBATCH_SIZE = 128\nNUM_WORKERS = 2\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nTARGET_DIM = 128\nW2V_WINDOW = 5\nW2V_MIN_COUNT = 5\n# =============================================================================\n\n# --- CLASSE MultimodalDataset (VERSION CORRIG√âE) ---\nclass MultimodalDataset(Dataset):\n    \"\"\"Dataset pour images + texte (avec gestion des arrays dans les tags)\"\"\"\n    \n    def __init__(self, df, img_dir, processor): \n        self.df = df\n        self.img_dir = img_dir\n        self.processor = processor\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        item_id = row['item_id']\n        img_path = os.path.join(self.img_dir, f\"{item_id}.jpg\")\n        \n        # --- 1. Extraction et Nettoyage du Texte (Logique Corrig√©e) ---\n        text_parts = []\n        \n        # Titre (Scalaire)\n        if 'title' in row and pd.notna(row['title']):\n            text_parts.append(str(row['title']))\n        \n        # Tags (Gestion des listes/arrays) - FIX ICI\n        if 'item_tags' in row:\n            tags_value = row['item_tags']\n            \n            # V√©rifier d'abord si c'est un array ou une liste\n            if isinstance(tags_value, (list, np.ndarray)):\n                # Pour les arrays/listes, v√©rifier la longueur\n                if len(tags_value) > 0:\n                    # Filtrer les valeurs non-NaN\n                    valid_tags = [str(tag) for tag in tags_value if pd.notna(tag)]\n                    if valid_tags:\n                        text_parts.append(\" \".join(valid_tags))\n            # Sinon, si c'est un scalaire, v√©rifier s'il est NaN\n            elif pd.notna(tags_value):\n                text_parts.append(str(tags_value))\n\n        # --- 2. Construction du Prompt Final ---\n        text = \" [SEP] \".join(text_parts).strip()\n        \n        if not text:\n             text = f\"Image of product with ID {item_id}\" \n        \n        # Image\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n        except:\n            image = Image.new('RGB', (224, 224), color='black')\n            \n        return {'text': text, 'image': image}\n\n\ndef collate_fn(batch):\n    \"\"\"Fonction de collation pour le DataLoader CLIP.\"\"\"\n    texts = [item['text'] for item in batch]\n    images = [item['image'] for item in batch]\n    inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n    return inputs\n\n\n# =============================================================================\n# FONCTIONS D'EMBEDDING\n# =============================================================================\n\ndef extract_content_embeddings(df_items, processor, model, target_dim):\n    \"\"\"√âtape B: Extraction CLIP + PCA 512D -> 128D.\"\"\"\n    \n    print(\"\\nüîÑ Extraction des embeddings de Contenu (CLIP)...\")\n    dataset = MultimodalDataset(df_items, IMG_DIR, processor) \n    dataloader = DataLoader(\n        dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False, collate_fn=collate_fn\n    )\n\n    all_embeddings = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Processing CLIP\"):\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            outputs = model(**batch)\n            \n            # Fusion image + texte\n            fused = (outputs.image_embeds + outputs.text_embeds) / 2.0\n            fused = fused / fused.norm(dim=-1, keepdim=True)\n            \n            all_embeddings.append(fused.cpu().numpy())\n\n    embeddings_512d = np.concatenate(all_embeddings, axis=0)\n    \n    # R√©duction PCA\n    pca = PCA(n_components=target_dim, random_state=42)\n    embeddings_128d = pca.fit_transform(embeddings_512d)\n    \n    # Normalisation finale\n    norms = np.linalg.norm(embeddings_128d, axis=1, keepdims=True)\n    embeddings_128d = embeddings_128d / (norms + 1e-8)\n    \n    return dict(zip(df_items['item_id'], embeddings_128d))\n\n\ndef extract_collaborative_embeddings(df_items, train_path, target_dim):\n    \"\"\"√âtape A: Entra√Ænement Word2Vec (Skip-gram) pour le signal collaboratif.\"\"\"\n    \n    print(\"\\nüîÑ Extraction des embeddings Collaboratifs (Word2Vec)...\")\n    \n    # Charger les s√©quences\n    df_train = pd.read_parquet(train_path)\n    sequences = [\n        [str(item_id) for item_id in seq] \n        for seq in df_train['item_seq'] \n        if isinstance(seq, (list, np.ndarray)) and len(seq) > 1\n    ]\n    \n    # Entra√Æner Word2Vec (Skip-gram est implicite par d√©faut)\n    model_w2v = Word2Vec(\n        sentences=sequences,\n        vector_size=target_dim,\n        window=W2V_WINDOW,\n        min_count=W2V_MIN_COUNT,\n        sg=1, # Skip-gram\n        workers=NUM_WORKERS,\n        seed=42\n    )\n    \n    collaborative_embeddings = {}\n    \n    # Extraire les embeddings pour tous les items du df_items\n    for item_id in tqdm(df_items['item_id'], desc=\"Extracting W2V\"):\n        item_id_str = str(item_id)\n        if item_id_str in model_w2v.wv:\n            collaborative_embeddings[item_id] = model_w2v.wv[item_id_str].astype(np.float32)\n        else:\n            # Utiliser un vecteur nul pour les items non vus (rares)\n            collaborative_embeddings[item_id] = np.zeros(target_dim, dtype=np.float32)\n            \n    # Normalisation\n    for item_id, emb in collaborative_embeddings.items():\n        collaborative_embeddings[item_id] = emb / (np.linalg.norm(emb) + 1e-8)\n            \n    return collaborative_embeddings\n\n\ndef fuse_embeddings(df_items, content_embs, collab_embs, target_dim):\n    \"\"\"√âtape C: Fusion par moyenne pond√©r√©e (alpha=0.5).\"\"\"\n    \n    fused_embeddings = {}\n    \n    for item_id in tqdm(df_items['item_id'], desc=\"Fusing Embeddings\"):\n        emb_content = content_embs.get(item_id, np.zeros(target_dim, dtype=np.float32))\n        emb_collab = collab_embs.get(item_id, np.zeros(target_dim, dtype=np.float32))\n        \n        # Simple fusion par moyenne\n        emb_fused = (emb_content + emb_collab) / 2.0\n        \n        # Normalisation finale\n        emb_fused = emb_fused / (np.linalg.norm(emb_fused) + 1e-8)\n        \n        fused_embeddings[item_id] = emb_fused\n        \n    return fused_embeddings\n\n# =============================================================================\n# MAIN\n# =============================================================================\nprint(f\"üöÄ TASK 1: FUSION MULTIMODAL + COLLABORATIF\")\nprint(\"=\"*80)\n\n# 1. CHARGEMENT ITEM_INFO\nprint(f\"\\n{'='*80}\\n√âTAPE 1: CHARGEMENT ITEM_INFO\\n{'='*80}\")\ntry:\n    df_items = pd.read_parquet(ITEM_INFO_PATH)\nexcept FileNotFoundError:\n    print(f\"‚ùå ERREUR: Fichier non trouv√© √† {ITEM_INFO_PATH}.\")\n    raise\n    \nprint(f\"‚úÖ {len(df_items)} items charg√©s\")\n\n# 2. INITIALISATION CLIP\nprint(f\"\\n{'='*80}\\n√âTAPE 2: INITIALISATION CLIP\\n{'='*80}\")\nmodel_name = \"openai/clip-vit-base-patch32\"\nprocessor = CLIPProcessor.from_pretrained(model_name)\nmodel = CLIPModel.from_pretrained(model_name)\nmodel.to(DEVICE)\nmodel.eval()\nprint(\"‚úÖ CLIP charg√©\")\n\n# 3. EXTRACTION D'EMBEDDINGS\nprint(f\"\\n{'='*80}\\n√âTAPE 3: EXTRACTION DES DEUX EMBEDDINGS (128D)\\n{'='*80}\")\n\n# 3.1. Content-Based (CLIP + PCA)\ncontent_embs = extract_content_embeddings(df_items, processor, model, TARGET_DIM)\n\n# 3.2. Collaborative-Based (Word2Vec)\ncollaborative_embs = extract_collaborative_embeddings(df_items, TRAIN_PATH, TARGET_DIM)\n\n\n# 4. FUSION\nprint(f\"\\n{'='*80}\\n√âTAPE 4: FUSION COLLABORATIF + CONTENT\\n{'='*80}\")\nfused_embs = fuse_embeddings(df_items, content_embs, collaborative_embs, TARGET_DIM)\nfused_emb_list = [fused_embs[item_id] for item_id in df_items['item_id']]\n\n# 5. UPDATE ITEM_INFO\nprint(f\"\\n{'='*80}\\n√âTAPE 5: UPDATE ITEM_INFO.PARQUET\\n{'='*80}\")\nif 'item_emb_d128' in df_items.columns:\n    df_items = df_items.drop(columns=['item_emb_d128'], errors='ignore')\n\ndf_items['item_emb_d128'] = fused_emb_list\n\n# Sauvegarder\ndf_items.to_parquet(OUTPUT_PATH, index=False)\nprint(f\"‚úÖ Fichier sauvegard√©: {OUTPUT_PATH}\")\nprint(f\"\\n{'='*80}\\nüéâ TASK 1 FUSIONN√âE TERMIN√âE !\\n{'='*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T19:09:17.620426Z","iopub.execute_input":"2025-12-14T19:09:17.620993Z","iopub.status.idle":"2025-12-14T19:31:44.265806Z","shell.execute_reply.started":"2025-12-14T19:09:17.620965Z","shell.execute_reply":"2025-12-14T19:31:44.265039Z"}},"outputs":[{"name":"stderr","text":"2025-12-14 19:09:27.396423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765739367.582660      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765739367.636986      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"üöÄ TASK 1: FUSION MULTIMODAL + COLLABORATIF\n================================================================================\n\n================================================================================\n√âTAPE 1: CHARGEMENT ITEM_INFO\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ 91718 items charg√©s\n\n================================================================================\n√âTAPE 2: INITIALISATION CLIP\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4d598194c68466498a67329262a764a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"457682c0a6eb49e8965cc39becd81053"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db7c4cad84ed463991f3f5b9f948ec39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82f0d1c459414915a2d7cec123600dab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e95f51c8b8141d992a0e214b20a553f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be7fb64b0d624c1284c84d3865fbaaa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c4614af576a465e855191f2680768f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996b976c86524877918a3ea33893ee57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e9a22287da441a996b0008939d57cc"}},"metadata":{}},{"name":"stdout","text":"‚úÖ CLIP charg√©\n\n================================================================================\n√âTAPE 3: EXTRACTION DES DEUX EMBEDDINGS (128D)\n================================================================================\n\nüîÑ Extraction des embeddings de Contenu (CLIP)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing CLIP:   0%|          | 0/717 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00688418d2e64c5da1f6231d0d79624a"}},"metadata":{}},{"name":"stdout","text":"\nüîÑ Extraction des embeddings Collaboratifs (Word2Vec)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting W2V:   0%|          | 0/91718 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd9b81e9ce749258e022a8a4cfbb2a4"}},"metadata":{}},{"name":"stdout","text":"\n================================================================================\n√âTAPE 4: FUSION COLLABORATIF + CONTENT\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fusing Embeddings:   0%|          | 0/91718 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf94013eedc3423bb779d369c03fa8fc"}},"metadata":{}},{"name":"stdout","text":"\n================================================================================\n√âTAPE 5: UPDATE ITEM_INFO.PARQUET\n================================================================================\n‚úÖ Fichier sauvegard√©: /kaggle/working/item_info_updated.parquet\n\n================================================================================\nüéâ TASK 1 FUSIONN√âE TERMIN√âE !\n================================================================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==================================================================================\n# MM-CTR TASK 2: DCN-DIN (Adapt√© √† Embeddings Fusionn√©s 128D)\n# ==================================================================================\n\nimport os\nimport gc\nimport sys\nimport zipfile\nimport subprocess\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 1. SETUP\n# ----------------------------------------------------------------------------------\ntry:\n    import polars as pl\nexcept ImportError:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polars\"])\n    import polars as pl\n\n# ==================================================================================\n# 2. CONFIGURATION (Utilise l'output de votre script de fusion)\n# ==================================================================================\n\nclass Config:\n    # CHEMINS (Utilise le chemin de sortie de votre script de fusion)\n    BASE_DIR = '/kaggle/working'\n    DATA_DIR = '/kaggle/input/embedingsctr'\n    \n    # ‚ö° MODIFICATION CL√â : Utilise votre fichier item_info_updated.parquet\n    EMB_PATH = BASE_DIR + '/item_info_updated.parquet' \n    \n    MODEL_SAVE_DIR = BASE_DIR\n    PRED_SAVE_DIR = BASE_DIR\n\n    # HYPERPARAM√àTRES\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # ‚ö° MODIFICATION CL√â : Dimension des Embeddings = 128D\n    EMBED_DIM = 128 \n    \n    SIDE_EMBED_DIM = 8 \n    CROSS_LAYERS = 3 \n    HIDDEN_UNITS = [128, 64]\n    DROPOUT = 0.1\n\n    # Entra√Ænement\n    BATCH_SIZE = 4096\n    LR = 1e-3\n    EPOCHS = 2 \n\nconfig = Config()\nos.makedirs(config.MODEL_SAVE_DIR, exist_ok=True)\nos.makedirs(config.PRED_SAVE_DIR, exist_ok=True)\nprint(f\"DEVICE: {config.DEVICE}, Embed Dim: {config.EMBED_DIM}\")\n\n# ==================================================================================\n# 3. ADVANCED LAYERS (Corrig√© __init__)\n# ==================================================================================\nclass Dice(nn.Module):\n    def __init__(self, num_features, dim=2):\n        super(Dice, self).__init__()\n        self.bn = nn.BatchNorm1d(num_features, eps=1e-9)\n        self.sig = nn.Sigmoid()\n        if dim == 2:\n            self.alpha = nn.Parameter(torch.zeros((num_features,)))\n        else:\n            self.alpha = nn.Parameter(torch.zeros((num_features, 1)))\n\n    def forward(self, x):\n        p = self.sig(self.bn(x))\n        return p * x + (1 - p) * self.alpha * x\n\nclass CrossNet(nn.Module):\n    \"\"\"Deep & Cross Network Layer (Corrig√© __init__)\"\"\"\n    def __init__(self, in_features, layer_num=2):\n        super().__init__()\n        self.layer_num = layer_num\n        self.kernels = nn.ParameterList([nn.Parameter(torch.Tensor(in_features, 1)) for _ in range(layer_num)])\n        self.biases = nn.ParameterList([nn.Parameter(torch.Tensor(in_features)) for _ in range(layer_num)])\n\n        for i in range(layer_num):\n            nn.init.xavier_normal_(self.kernels[i])\n            nn.init.zeros_(self.biases[i])\n\n    def forward(self, inputs):\n        x_0 = inputs.unsqueeze(2)\n        x_l = x_0\n        for i in range(self.layer_num):\n            xl_w = torch.matmul(x_l.transpose(1, 2), self.kernels[i])\n            dot_ = torch.matmul(x_0, xl_w)\n            x_l = dot_ + self.biases[i].unsqueeze(1) + x_l\n        return x_l.squeeze(2)\n# ==================================================================================\n# 4. DATA LOADING (Adapt√© √† item_emb_d128)\n# ==================================================================================\ndef load_assets():\n    print(\"  Loading FUSED Embeddings (128D) from custom file...\")\n    try:\n        # NOTE: Pandas est utilis√© ici car Polars a des probl√®mes avec les arrays NumPy\n        df_emb = pd.read_parquet(config.EMB_PATH) \n    except Exception as e:\n        print(f\"ERREUR: Impossible de lire le fichier d'embeddings √† {config.EMB_PATH}.\")\n        print(\"Assurez-vous d'avoir ex√©cut√© le script de fusion avant.\")\n        raise e\n\n    # Cr√©ation de la carte ID -> Index\n    real_ids = df_emb['item_id'].to_list()\n    id_to_idx = {real_id: i + 1 for i, real_id in enumerate(real_ids)}\n\n    # Extraction des embeddings fusionn√©s (128D)\n    # ‚ö° MODIFICATION : On utilise 'item_emb_d128' √† la place de v1/v2\n    combined_emb = np.stack(df_emb['item_emb_d128'].to_numpy(), dtype=np.float32)\n\n    # Ajout d'une ligne de padding (index 0)\n    padding_row = np.zeros((1, config.EMBED_DIM), dtype=np.float32)\n    final_matrix = np.vstack([padding_row, combined_emb])\n\n    print(f\" Combined Matrix Shape: {final_matrix.shape}\")\n    del df_emb, combined_emb\n    gc.collect()\n    return torch.tensor(final_matrix), id_to_idx\n\ntry:\n    PRETRAINED_WEIGHTS, ID_MAP = load_assets()\nexcept Exception as e:\n    print(f\"Arr√™t d√ª √† l'erreur de chargement: {e}\")\n    sys.exit(1)\n\n\n# ==================================================================================\n# 4. DATA LOADING (Corrig√© __init__ pour l'extraction de l'ID de test)\n# ==================================================================================\n# ... (load_assets et PRETRAINED_WEIGHTS restent inchang√©s) ...\n\n# ==================================================================================\n# 4. DATA LOADING (Corrig√© __init__ pour l'extraction de l'ID de test V2)\n# ==================================================================================\n# ... (load_assets et PRETRAINED_WEIGHTS restent inchang√©s) ...\n\nclass RichDataset(Dataset):\n    def __init__(self, parquet_path, id_map, is_test=False):\n        print(f\" Reading {os.path.basename(parquet_path)}...\")\n        df = pl.read_parquet(parquet_path)\n\n        def map_ids(id_array):\n            return np.array([id_map.get(x, 0) for x in id_array], dtype=np.int32)\n\n        self.target = map_ids(df['item_id'].to_numpy())\n        \n        seq_matrix = np.stack(df['item_seq'].to_numpy())\n        self.history = map_ids(seq_matrix.flatten()).reshape(seq_matrix.shape)\n        \n        self.likes = df['likes_level'].to_numpy().astype(np.int32) \n        self.views = df['views_level'].to_numpy().astype(np.int32)\n\n        self.is_test = is_test\n        \n        if not is_test:\n            self.label = df['label'].to_numpy().astype(np.float32)\n            self.ids = None\n        else:\n            self.label = np.zeros(len(df), dtype=np.float32)\n            \n            # ‚ö° FIX CRITIQUE V2: Cr√©e l'ID s√©quentiel directement √† partir de la longueur du DF.\n            # C'est la m√©thode la plus s√ªre et n'interf√®re pas avec les colonnes Polars.\n            self.ids = np.arange(len(df), dtype=np.int32)\n            \n        del df, seq_matrix\n        gc.collect()\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, idx):\n        return (self.history[idx], self.target[idx], self.likes[idx], self.views[idx], self.label[idx])\n# ... (Le reste du code reste inchang√©) ...\n  \n\n# ... (Le reste du code reste inchang√©) ...\n     \n# ==================================================================================\n# 5. DCN MODEL (Utilise EMBED_DIM = 128)\n# ==================================================================================\nclass DCN_DIN(nn.Module):\n    def __init__(self, pretrained_weights):\n        super().__init__()\n        num_items, embed_dim = pretrained_weights.shape # embed_dim = 128\n\n        self.item_embedding = nn.Embedding(num_items, embed_dim, padding_idx=0)\n        self.item_embedding.weight.data.copy_(pretrained_weights)\n        self.item_embedding.weight.requires_grad = True\n\n        self.likes_embedding = nn.Embedding(20, config.SIDE_EMBED_DIM)\n        self.views_embedding = nn.Embedding(20, config.SIDE_EMBED_DIM)\n\n        att_input_dim = embed_dim * 4\n        self.att_mlp = nn.Sequential(\n            nn.Linear(att_input_dim, 80),\n            nn.Sigmoid(),\n            nn.Linear(80, 40),\n            nn.Sigmoid(),\n            nn.Linear(40, 1)\n        )\n\n        self.input_dim = embed_dim * 2 + config.SIDE_EMBED_DIM * 2\n        self.cross_net = CrossNet(self.input_dim, layer_num=config.CROSS_LAYERS)\n\n        deep_layers = []\n        curr_dim = self.input_dim\n        for hidden in config.HIDDEN_UNITS:\n            deep_layers.append(nn.Linear(curr_dim, hidden))\n            deep_layers.append(Dice(hidden))\n            deep_layers.append(nn.Dropout(config.DROPOUT))\n            curr_dim = hidden\n        self.deep_net = nn.Sequential(*deep_layers)\n\n        self.final_linear = nn.Linear(curr_dim + self.input_dim, 1)\n\n    def attention(self, target, history, mask):\n        # ... (Logique DIN inchang√©e)\n        seq_len = history.size(1)\n        target_tile = target.expand(-1, seq_len, -1)\n        inp = torch.cat([target_tile, history, target_tile - history, target_tile * history], dim=-1)\n        scores = self.att_mlp(inp).masked_fill(mask.unsqueeze(-1) == 0, -1e9)\n        return (torch.softmax(scores, dim=1) * history).sum(dim=1)\n\n    def forward(self, history, target, likes, views):\n        # ... (Logique Forward inchang√©e)\n        hist_emb = self.item_embedding(history)\n        target_emb = self.item_embedding(target).unsqueeze(1)\n\n        mask = (history != 0)\n        user_interest = self.attention(target_emb, hist_emb, mask)\n\n        features = torch.cat([\n            target_emb.squeeze(1),\n            user_interest,\n            self.likes_embedding(likes),\n            self.views_embedding(views)\n        ], dim=1)\n\n        cross_out = self.cross_net(features)\n        deep_out = self.deep_net(features)\n        stack = torch.cat([cross_out, deep_out], dim=1)\n        return self.final_linear(stack).squeeze()\n# ==================================================================================\n# 6. TRAINING (Utilise les fichiers train.parquet et valid (1).parquet)\n# ==================================================================================\ndef train_engine():\n    # Chemins adapt√©s aux fichiers du dossier 'embedingsctr'\n    train_ds = RichDataset(os.path.join(config.DATA_DIR, 'train.parquet'), ID_MAP)\n    valid_ds = RichDataset(os.path.join(config.DATA_DIR, 'valid (1).parquet'), ID_MAP)\n\n    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n    valid_loader = DataLoader(valid_ds, batch_size=config.BATCH_SIZE*2, shuffle=False)\n\n    model = DCN_DIN(PRETRAINED_WEIGHTS).to(config.DEVICE)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=config.LR, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=1)\n\n    best_auc = 0.0\n    patience = 0\n\n    print(\"\\n STARTING TRAINING \")\n    for epoch in range(config.EPOCHS):\n        model.train()\n        total_loss = 0\n\n        for history, target, likes, views, label in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.EPOCHS}\"):\n            history, target = history.to(config.DEVICE).long(), target.to(config.DEVICE).long()\n            likes, views = likes.to(config.DEVICE).long(), views.to(config.DEVICE).long()\n            label = label.to(config.DEVICE).float()\n\n            optimizer.zero_grad()\n            logits = model(history, target, likes, views)\n            loss = criterion(logits, label)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            total_loss += loss.item()\n\n        model.eval()\n        preds, labels = [], []\n        with torch.no_grad():\n            for history, target, likes, views, label in valid_loader:\n                history, target = history.to(config.DEVICE).long(), target.to(config.DEVICE).long()\n                likes, views = likes.to(config.DEVICE).long(), views.to(config.DEVICE).long()\n                logits = model(history, target, likes, views)\n                preds.extend(torch.sigmoid(logits).cpu().numpy())\n                labels.extend(label.numpy())\n\n        val_auc = roc_auc_score(labels, preds)\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"üìä Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f} | Val AUC={val_auc:.4f} | LR={current_lr:.1e}\")\n\n        scheduler.step(val_auc)\n\n        if val_auc > best_auc:\n            best_auc = val_auc\n            torch.save(model.state_dict(), os.path.join(config.MODEL_SAVE_DIR, 'dcn_best.pt'))\n            print(f\"    Best Model Saved! AUC: {best_auc:.4f}\")\n            patience = 0\n        else:\n            patience += 1\n            if patience >= 3:\n                print(\"    Early Stopping\")\n                break\n\n    print(f\"\\n Training Complete! Best AUC: {best_auc:.4f}\")\n    return model\n\ndef generate_submission(model):\n    print(\"\\nüîÆ Generating Predictions...\")\n    # Chemin adapt√© aux fichiers du dossier 'embedingsctr'\n    test_ds = RichDataset(os.path.join(config.DATA_DIR, 'test (1).parquet'), ID_MAP, is_test=True) \n    test_loader = DataLoader(test_ds, batch_size=config.BATCH_SIZE*2, shuffle=False)\n\n    model_path = os.path.join(config.MODEL_SAVE_DIR, 'dcn_best.pt')\n    if os.path.exists(model_path):\n        model.load_state_dict(torch.load(model_path, map_location=config.DEVICE))\n    \n    model.eval()\n\n    all_preds = []\n    with torch.no_grad():\n        for history, target, likes, views, _ in tqdm(test_loader):\n            history, target = history.to(config.DEVICE).long(), target.to(config.DEVICE).long()\n            likes, views = likes.to(config.DEVICE).long(), views.to(config.DEVICE).long()\n            logits = model(history, target, likes, views)\n            all_preds.extend(torch.sigmoid(logits).cpu().numpy())\n\n    # Cr√©ation du DataFrame de soumission avec les 4 colonnes requises\n    df = pd.DataFrame({\n        'ID': test_ds.ids, \n        'Task1': 0.0,\n        'Task2': 0.0,\n        'Task1&2': all_preds # Place la pr√©diction dans Task1&2 (CTR)\n    })\n    \n    path = os.path.join(config.PRED_SAVE_DIR, 'prediction.csv')\n    # ‚ö° FIX : Enregistrer les pr√©dictions dans Task1&2, Task1 et Task2 restent √† 0\n    df.to_csv(path, index=False, float_format='%.8f') \n\n    zip_path = os.path.join(config.PRED_SAVE_DIR, 'submission_task2.zip')\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n        z.write(path, 'prediction.csv')\n    \n    print(f\" Soumission pr√™te: {zip_path}\")\n    print(f\" CSV saved at: {path}\")\n    return zip_path\n# ==================================================================================\n# 7. RUN EVERYTHING\n# ==================================================================================\nif __name__ == \"__main__\":\n    print(\"=\"*70)\n    print(\" MM-CTR TASK 2 - DCN-DIN (avec vos embeddings 128D) STARTING\")\n    print(\"=\"*70)\n    \n    gc.collect()\n    if config.DEVICE == 'cuda':\n        torch.cuda.empty_cache()\n    \n    try:\n        model = train_engine()\n        submission_path = generate_submission(model)\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\" ALL DONE!\")\n        print(f\" T√©l√©chargez ce fichier et soumettez-le: {submission_path}\")\n        print(\"=\"*70)\n    except Exception as e:\n        print(f\"\\n‚ùå Une erreur s'est produite pendant l'entra√Ænement ou la pr√©diction : {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T19:41:43.233673Z","iopub.execute_input":"2025-12-14T19:41:43.234474Z","iopub.status.idle":"2025-12-14T19:45:14.408277Z","shell.execute_reply.started":"2025-12-14T19:41:43.234444Z","shell.execute_reply":"2025-12-14T19:45:14.407507Z"}},"outputs":[{"name":"stdout","text":"DEVICE: cuda, Embed Dim: 128\n  Loading FUSED Embeddings (128D) from custom file...\n Combined Matrix Shape: (91719, 128)\n======================================================================\n MM-CTR TASK 2 - DCN-DIN (avec vos embeddings 128D) STARTING\n======================================================================\n Reading train.parquet...\n Reading valid (1).parquet...\n\n STARTING TRAINING \n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 879/879 [01:07<00:00, 13.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"üìä Epoch 1: Loss=0.1661 | Val AUC=0.8108 | LR=1.0e-03\n    Best Model Saved! AUC: 0.8108\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 879/879 [01:06<00:00, 13.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"üìä Epoch 2: Loss=0.0384 | Val AUC=0.8517 | LR=1.0e-03\n    Best Model Saved! AUC: 0.8517\n\n Training Complete! Best AUC: 0.8517\n\nüîÆ Generating Predictions...\n Reading test (1).parquet...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:04<00:00, 10.62it/s]\n","output_type":"stream"},{"name":"stdout","text":" Soumission pr√™te: /kaggle/working/submission_task2.zip\n CSV saved at: /kaggle/working/prediction.csv\n\n======================================================================\n ALL DONE!\n T√©l√©chargez ce fichier et soumettez-le: /kaggle/working/submission_task2.zip\n======================================================================\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"#### ","metadata":{}}]}